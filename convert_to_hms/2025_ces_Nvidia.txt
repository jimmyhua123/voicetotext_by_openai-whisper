[0:00:23 - 0:00:24]  Do you like my jacket?
[0:00:24 - 0:00:30]  I thought I'd go the other way from Gary Shapiro.
[0:00:32 - 0:00:34]  I'm in Las Vegas after all.
[0:00:34 - 0:00:37]  If this doesn't work out, if all of you object,
[0:00:40 - 0:00:41]  get used to it.
[0:00:41 - 0:00:44]  I think you have to let this sink in.
[0:00:46 - 0:00:48]  In another hour or so, you're going to feel good about it.
[0:00:53 - 0:00:55]  Welcome to NVIDIA.
[0:00:56 - 0:00:59]  In fact, you're inside NVIDIA's digital twin.
[0:01:00 - 0:01:03]  We're going to take you to NVIDIA.
[0:01:04 - 0:01:05]  Ladies and gentlemen, welcome to NVIDIA.
[0:01:09 - 0:01:12]  You're inside our digital twin.
[0:01:15 - 0:01:18]  Everything here is generated by AI.
[0:01:21 - 0:01:24]  It has been an extraordinary journey, extraordinary year.
[0:01:25 - 0:01:28]  And it started in 1993.
[0:01:29 - 0:01:30]  Ready, go!
[0:01:31 - 0:01:37]  With NVIDIA, we wanted to build computers that can do things
[0:01:37 - 0:01:38]  that normal computers couldn't.
[0:01:38 - 0:01:44]  And NVIDIA made it possible to have a game console in your PC.
[0:01:45 - 0:01:48]  Our programming architecture was called UDA,
[0:01:49 - 0:01:52]  missing the letter C until a little while later,
[0:01:52 - 0:01:55]  but UDA, Unified Device Architecture.
[0:01:56 - 0:01:59]  And the first developer for UDA
[0:01:59 - 0:02:02]  and the first application that I've worked on UDA
[0:02:02 - 0:02:05]  was Sega's virtual fighter.
[0:02:05 - 0:02:12]  Six years later, we invented in 1999 the programmable GPU.
[0:02:13 - 0:02:20]  And it started 20 years, 20 plus years of incredible advance
[0:02:20 - 0:02:23]  in this incredible processor called the GPU.
[0:02:24 - 0:02:27]  It made modern computer graphics possible.
[0:02:28 - 0:02:31]  And now, 30 years later,
[0:02:31 - 0:02:35]  Sega's virtual fighter is completely cinematic.
[0:02:37 - 0:02:40]  This is the new virtual fighter project that's coming.
[0:02:41 - 0:02:41]  I just can't wait.
[0:02:42 - 0:02:43]  Absolutely incredible.
[0:02:45 - 0:02:48]  Six years after that, six years after 1999,
[0:02:49 - 0:02:55]  we invented CUDA so that we could explain or express
[0:02:55 - 0:02:59]  the programmability of our GPUs to a rich set of algorithms
[0:02:59 - 0:03:01]  like a benefit from it.
[0:03:01 - 0:03:05]  CUDA initially was difficult to explain.
[0:03:06 - 0:03:07]  And it took years, in fact.
[0:03:07 - 0:03:10]  It took approximately six years.
[0:03:11 - 0:03:15]  Somehow, six years later, six years later or so,
[0:03:20 - 0:03:23]  2012, Alex Khrushchevsky, Ilya Suskovur
[0:03:23 - 0:03:26]  and Jeff Hinton discovered CUDA,
[0:03:26 - 0:03:30]  used it to process AlexNet,
[0:03:30 - 0:03:32]  and the rest of it is history.
[0:03:33 - 0:03:36]  AI has been advancing at an incredible pace since.
[0:03:37 - 0:03:38]  Started with Perception AI.
[0:03:39 - 0:03:42]  We now can understand images and words and sounds.
[0:03:43 - 0:03:47]  To generative AI, we can generate images and text and sounds.
[0:03:48 - 0:03:51]  And now, agentic AI.
[0:03:51 - 0:03:56]  AI's that can perceive, reason, plan, and act.
[0:03:57 - 0:04:00]  And then the next phase, some of which we'll talk about tonight,
[0:04:00 - 0:04:02]  physical AI, 2012.
[0:04:03 - 0:04:08]  Now, magically, 2018, something happened.
[0:04:09 - 0:04:09]  That was pretty incredible.
[0:04:11 - 0:04:15]  Google's transformer was released as BERT,
[0:04:15 - 0:04:19]  and the world of AI really took off.
[0:04:20 - 0:04:22]  Transformers, as you know,
[0:04:22 - 0:04:25]  completely changed the landscape for artificial intelligence.
[0:04:26 - 0:04:29]  In fact, it completely changed the landscape
[0:04:29 - 0:04:30]  for computing altogether.
[0:04:31 - 0:04:36]  We recognized properly that AI was not just a new application
[0:04:36 - 0:04:39]  with a new business opportunity,
[0:04:40 - 0:04:43]  but AI, more importantly, machine learning,
[0:04:43 - 0:04:47]  enabled by transformers, was going to fundamentally change
[0:04:47 - 0:04:48]  how computing works.
[0:04:49 - 0:04:56]  And today, computing is revolutionized in every single layer.
[0:04:56 - 0:05:01]  From hand-coding, instructions that run on CPUs
[0:05:01 - 0:05:03]  to create software tools that humans use,
[0:05:04 - 0:05:08]  we now have machine learning that creates and optimizes
[0:05:08 - 0:05:11]  new networks that processes on GPUs
[0:05:11 - 0:05:13]  and creates artificial intelligence.
[0:05:14 - 0:05:17]  Every single layer of the technology stack
[0:05:17 - 0:05:19]  has been completely changed.
[0:05:20 - 0:05:23]  An incredible transformation in just 12 years.
[0:05:25 - 0:05:30]  Well, we can now understand information of just about any modality.
[0:05:31 - 0:05:34]  Surely you've seen text and images and sounds and things like that.
[0:05:35 - 0:05:37]  But not only can we understand those,
[0:05:38 - 0:05:40]  we can understand amino acids, we can understand physics.
[0:05:41 - 0:05:45]  We understand them, we can translate them, and generate them.
[0:05:46 - 0:05:48]  The applications are just completely endless.
[0:05:49 - 0:05:52]  In fact, almost any AI application that you see out there,
[0:05:53 - 0:05:55]  what modality is the input that it learned from,
[0:05:56 - 0:05:59]  what modality of information did it translate to,
[0:05:59 - 0:06:02]  and what modality of information is it generating
[0:06:02 - 0:06:04]  if you ask these three fundamental questions,
[0:06:05 - 0:06:08]  just about every single application could be inferred.
[0:06:08 - 0:06:11]  And so when you see application after applications
[0:06:11 - 0:06:15]  that are AI-driven, AI-native,
[0:06:15 - 0:06:18]  at the core of it, this fundamental concept is there.
[0:06:19 - 0:06:21]  Machine learning has changed how every application
[0:06:21 - 0:06:25]  is going to be built, how computing will be done,
[0:06:25 - 0:06:27]  and the possibilities behind.
[0:06:28 - 0:06:30]  Well, GPUs,
[0:06:31 - 0:06:33]  G-Force, in a lot of ways,
[0:06:35 - 0:06:37]  all of this with AI is the house
[0:06:37 - 0:06:38]  that G-Force built.
[0:06:39 - 0:06:42]  G-Force enabled AI to reach the masses.
[0:06:43 - 0:06:46]  And now, AI is coming home to G-Force.
[0:06:47 - 0:06:51]  There are so many things that you can't do without AI.
[0:06:52 - 0:06:54]  Let me show you some of it now.
[0:07:52 - 0:07:54]  The AI is going to be built in a way that is going to be built.
[0:08:24 - 0:08:24]  The AI is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to be built in a way that is going to
[0:08:29 - 0:08:32]  be built in a way that was real time computer graphics.
[0:08:37 - 0:08:39]  That was real time computer graphics.
[0:08:39 - 0:08:50]  No computer graphics researcher, no computer scientists would have told you that it is possible for us to raytrace every single pixel at this point.
[0:08:51 - 0:08:56]  raytracing is a simulation of light. The amount of geometry that you saw was absolutely insane.
[0:08:57 - 0:09:03]  It would have been impossible without artificial intelligence. There are two fundamental things that we did.
[0:09:04 - 0:09:10]  We used of course, programmable shading and raytraced acceleration to produce incredibly beautiful pixels.
[0:09:11 - 0:09:20]  But then we have artificial intelligence to be conditioned to be controlled by that pixel to generate
[0:09:20 - 0:09:28]  a whole bunch of other pixels. Not only is it able to generate other pixels spatially because it is aware of what the colors should be.
[0:09:29 - 0:09:40]  It has been trained on a supercomputer back in Nvidia and so the neural network that is running on the GPU can infer and predict the pixels that we did not render.
[0:09:41 - 0:09:49]  Not only can we do that, it is called DLSS. The latest generation of DLSS also generates beyond frames.
[0:09:49 - 0:09:55]  It can predict the future, generating three additional frames for every frame that we calculate.
[0:09:56 - 0:10:03]  What you saw, if we just said four frames of what you saw, because we are going to render one frame and generate three,
[0:10:03 - 0:10:16]  if I said four frames at full HD, 4K, that is 33 million pixels or so, out of that 33 million pixels, we computed only two.
[0:10:18 - 0:10:28]  It is an absolute miracle that we can computationally, computationally using programmable shaders and our raytraced engine, raytracing engine,
[0:10:28 - 0:10:34]  to compute two million pixels and have AI predict all of the other 33.
[0:10:35 - 0:10:42]  And as a result, we are able to render at incredibly high performance because AI does a lot less computation.
[0:10:42 - 0:10:51]  It takes an enormous amount of training to produce that, but once you train it, the generation is extremely efficient.
[0:10:52 - 0:10:59]  So this is one of the incredible capabilities of artificial intelligence and that is why there are so many amazing things that are happening.
[0:10:59 - 0:11:06]  We used GeForce to enable artificial intelligence and now artificial intelligence is revolutionizing GeForce.
[0:11:06 - 0:11:15]  Everyone, today we are announcing our next generation, the RTX Blackwell family. Let's take a look.
[0:12:11 - 0:12:14]  Here it is.
[0:12:15 - 0:12:32]  Our brand new GeForce RTX 50 series, Blackwell architecture, the GPU is just a beast, 92 billion transistors, 4,000 tops,
[0:12:33 - 0:12:42]  4 petaflops of AI, 3 times higher than the last generation ADA, and we need all of it to generate those pixels that I showed you.
[0:12:44 - 0:12:52]  380 raytracing teraflops so that we could, for the pixels that we have to compute, compute the most beautiful image you possibly can.
[0:12:52 - 0:13:02]  Of course, 125 shader teraflops, there is a concurrent shader teraflops as well as an integer unit of equal performance.
[0:13:02 - 0:13:15]  So two dual shaders, one is for floating point, one is for integer, g7 memory from micron, 1.8 terabytes per second, twice the performance of our last generation.
[0:13:15 - 0:13:21]  And we now have the ability to intermix AI workloads with computer graphics workloads.
[0:13:22 - 0:13:30]  And one of the amazing things about this generation is the programmable shader is also able to now process neural networks.
[0:13:30 - 0:13:40]  So the shader is able to carry these neural networks and as result we invented neural texture compression and neural material shading.
[0:13:40 - 0:13:52]  As a result of that, you get these amazingly beautiful images that are only possible because we use AI to learn the texture, learn the compression algorithm, and as result get extraordinary results.
[0:13:53 - 0:14:02]  So this is the brand new RTX Blackwell 50-Monday.
[0:14:05 - 0:14:11]  Now, even the mechanical design is a miracle.
[0:14:11 - 0:14:13]  Look at this, it's got two fans.
[0:14:14 - 0:14:17]  This whole graphics card is just one giant fan.
[0:14:18 - 0:14:21]  So the question is, where's the graphics card? Is it literally this big?
[0:14:22 - 0:14:25]  The voltage regular to design is state of the art.
[0:14:26 - 0:14:27]  Incredible design.
[0:14:28 - 0:14:29]  The engineering team did a great job.
[0:14:30 - 0:14:30]  So here it is.
[0:14:31 - 0:14:31]  Thank you.
[0:14:33 - 0:14:34]  APPLAUSE
[0:14:37 - 0:14:40]  OK, those are the speeds and feeds of how does it compare?
[0:14:43 - 0:14:47]  Well, this is RTX 4090.
[0:14:48 - 0:14:50]  I know, I know many of you have one.
[0:14:53 - 0:14:54]  I know it.
[0:14:54 - 0:14:56]  Look, it's $1,599.
[0:14:56 - 0:15:00]  It is one of the best investments you could possibly make.
[0:15:02 - 0:15:12]  For $1599, you bring it home to your $10,000 PC Entertainment Command Center.
[0:15:12 - 0:15:13]  Isn't that right?
[0:15:14 - 0:15:15]  Don't tell me that's not true.
[0:15:16 - 0:15:16]  Don't be ashamed.
[0:15:18 - 0:15:20]  It's liquid-cooled.
[0:15:21 - 0:15:23]  Fancy lights all over it.
[0:15:24 - 0:15:26]  You lock it when you leave.
[0:15:28 - 0:15:30]  It's the modern home theater.
[0:15:31 - 0:15:32]  It makes perfect sense.
[0:15:32 - 0:15:35]  And now, for $1500 and $1900, $1599,
[0:15:36 - 0:15:39]  you get to upgrade that and then turbo-charge the living data lights out of it.
[0:15:39 - 0:15:43]  Well, now, with the Blackwell family, RTX 5070.
[0:15:44 - 0:15:47]  4090 performance at 549.
[0:15:55 - 0:16:02]  In possible without artificial intelligence, impossible without the four tops, four
[0:16:02 - 0:16:08]  tarops of AI tensor cores, impossible without the G7 memories.
[0:16:09 - 0:16:15]  OK, so $5070, $4090 performance, $549, and here's the whole family.
[0:16:15 - 0:16:21]  Starting from $5070, all the way up to $5090, $5090, twice the performance of $4090.
[0:16:25 - 0:16:30]  Starting, of course, we're producing at very large scale availability starting January.
[0:16:31 - 0:16:41]  Well, it is incredible, but we managed to put these in gigantic performance GPUs into a laptop.
[0:16:42 - 0:16:43]  This is a 5070 laptop.
[0:16:44 - 0:16:50]  For $1299, this 5070 laptop has a $4090 performance.
[0:16:51 - 0:16:52]  I think there's one here somewhere.
[0:16:54 - 0:16:56]  Let me show you this.
[0:16:58 - 0:17:00]  Look at this thing.
[0:17:03 - 0:17:04]  Here's only so many pockets.
[0:17:06 - 0:17:08]  Ladies and gentlemen, Janine Paul.
[0:17:12 - 0:17:13]  Can you imagine?
[0:17:14 - 0:17:16]  You got this incredible graphics card here, Blackwell.
[0:17:16 - 0:17:18]  We're going to shrink it and put it in there.
[0:17:19 - 0:17:20]  Does that make any sense?
[0:17:22 - 0:17:25]  Well, you can't do that with our artificial intelligence.
[0:17:25 - 0:17:30]  The reason for that is because we're generating most of the pixels using our tensor cores.
[0:17:30 - 0:17:37]  So we rate trace only the pixels we need and we generate using artificial intelligence all the other pixels we have.
[0:17:37 - 0:17:41]  As a result, the energy efficiency is just off the charts.
[0:17:42 - 0:17:48]  The future of computer graphics is neural rendering, the fusion of artificial intelligence and computer graphics.
[0:17:49 - 0:17:54]  And what's really amazing is, oh, here we go.
[0:17:54 - 0:17:55]  Thank you.
[0:17:56 - 0:17:58]  This is a surprisingly kinetic keynote.
[0:18:00 - 0:18:04]  And what's really amazing is the family of GPUs we're going to put it here.
[0:18:05 - 0:18:10]  And so the 5090, the 5090, will fit into a laptop, a thin laptop.
[0:18:10 - 0:18:13]  That last laptop was 14.9 millimeters.
[0:18:14 - 0:18:17]  You got a 5080, 5070 Ti and 5070.
[0:18:18 - 0:18:23]  So ladies and gentlemen, the RTX Blackwell family.
[0:18:29 - 0:18:38]  Well, G-Force brought AI to the world, democratized AI.
[0:18:39 - 0:18:43]  Now AI has come back and revolutionized G-Force.
[0:18:43 - 0:18:45]  Let's talk about artificial intelligence.
[0:18:46 - 0:18:47]  Let's go to somewhere else at NVIDIA.
[0:18:52 - 0:18:54]  This is literally office.
[0:18:54 - 0:18:56]  This is literally NVIDIA headquarters.
[0:18:58 - 0:19:01]  Okay, so let's talk about AI.
[0:19:02 - 0:19:11]  The industry is chasing and racing to scale artificial intelligence and artificial intelligence.
[0:19:12 - 0:19:15]  And the scaling law is a powerful model.
[0:19:15 - 0:19:23]  It's an empirical law that has been observed and demonstrated by researchers and industry over several generations.
[0:19:24 - 0:19:33]  And the scaling law says that the more data you have, the training data that you have, the larger model that you have,
[0:19:33 - 0:19:42]  and the more compute that you apply to it, therefore, the more effective or the more capable your model will become.
[0:19:43 - 0:19:45]  And so the scaling law continues.
[0:19:46 - 0:19:55]  What's really amazing is that now we're moving towards, of course, in the internet it's producing about twice the amount of data every single year as a bit last year.
[0:19:55 - 0:20:04]  I think in the next couple of years we'll produce, humanity will produce more data than all of humanity has ever produced since the beginning.
[0:20:05 - 0:20:10]  And so we're still producing a gigantic amount of data and it's becoming multi-modal.
[0:20:11 - 0:20:13]  Video and images and sound.
[0:20:13 - 0:20:20]  All of that data could be used to train the fundamental knowledge, the foundational knowledge of an AI.
[0:20:20 - 0:20:26]  But there are, in fact, two other scaling laws that has now emerged.
[0:20:26 - 0:20:28]  And it's somewhat intuitive.
[0:20:28 - 0:20:32]  The second scaling law is post-training scaling law.
[0:20:32 - 0:20:37]  Post-training scaling law uses technologies, techniques like reinforcement learning, human feedback.
[0:20:38 - 0:20:42]  Basically, the AI produces and generates answers.
[0:20:42 - 0:20:48]  The human, based on a human query, the human, then, of course, gives a feedback.
[0:20:48 - 0:20:59]  It's much more complicated than that, but that reinforcement learning system with a fair number of very high quality prompts causes the AI to refine its skills.
[0:21:00 - 0:21:03]  It could fine tune its skills for particular domains.
[0:21:03 - 0:21:07]  It could be better at solving math problems, better at reasoning, so on and so forth.
[0:21:07 - 0:21:16]  And so it's essentially like having a mentor or having a coach give you feedback after you're done going to school.
[0:21:17 - 0:21:20]  And so you get tests, you get feedback, you improve yourself.
[0:21:20 - 0:21:25]  We also have reinforcement learning AI feedback, and we have synthetic data generation.
[0:21:26 - 0:21:33]  These techniques are rather akin to, if you will, self-practice.
[0:21:33 - 0:21:39]  You know the answer to a particular problem, and you continue to try it until you get it right.
[0:21:40 - 0:21:48]  And so an AI could be presented with a very complicated and a difficult problem that is verifiable functionally.
[0:21:48 - 0:21:55]  And it has an answer that we understand, maybe proving a theorem, maybe solving a geometry problem.
[0:21:55 - 0:22:04]  And so these problems would cause the AI to produce answers, and using reinforcement learning, you would learn how to improve itself.
[0:22:04 - 0:22:06]  That's called post-training.
[0:22:06 - 0:22:12]  Post-training requires enormous amount of computation, but the end result produces incredible models.
[0:22:13 - 0:22:15]  We now have a third scaling law.
[0:22:15 - 0:22:20]  And the third scaling law has to do with what's called test time scaling.
[0:22:20 - 0:22:24]  Test time scaling is basically when you're being used.
[0:22:24 - 0:22:32]  When you're using the AI, the AI has the ability to now apply a different resource allocation.
[0:22:32 - 0:22:42]  Instead of improving its parameters, now it's focused on deciding how much computation to use to produce the answers it wants to produce.
[0:22:43 - 0:22:47]  Reasoning is a way of thinking about this, long thinking is a way to think about this.
[0:22:48 - 0:22:53]  Instead of a direct inference or one shot answer, you might reason about it.
[0:22:53 - 0:22:55]  You might break down the problem into multiple steps.
[0:22:56 - 0:23:04]  You might generate multiple ideas, and evaluate your AI system, which evaluate which one of the ideas that you generated was the best one.
[0:23:05 - 0:23:08]  Maybe it solves the problem step by step, so on and so forth.
[0:23:08 - 0:23:13]  Now test time scaling has proven to be incredibly effective.
[0:23:13 - 0:23:26]  You're watching this sequence of technology and all of these scaling laws emerge as we see incredible achievements from chat GPT to 01 to 03.
[0:23:27 - 0:23:36]  Now Gemini Pro, all of these systems are going through this journey step by step, a pre-training to post-training to test time scaling.
[0:23:36 - 0:23:41]  Well the amount of computation that we need, of course, is incredible.
[0:23:41 - 0:23:51]  And we would like, in fact, that society has the ability to scale the amount of computation to produce more and more novel and better intelligence.
[0:23:52 - 0:23:58]  Intelligence, of course, is the most valuable asset that we have, and it can be applied to solve a lot of very challenging problems.
[0:23:59 - 0:24:09]  And so scaling law, it's driving enormous demand for Nvidia computing, it's driving enormous demand for this incredible chip we call Blackwell.
[0:24:10 - 0:24:11]  Let's take a look at Blackwell.
[0:24:12 - 0:24:14]  Well Blackwell is in full production.
[0:24:17 - 0:24:19]  It is incredible what it looks like.
[0:24:20 - 0:24:26]  So first of all, there are some, every single cloud service provider now have systems up and running.
[0:24:26 - 0:24:34]  We have systems here from about 15, 15 computer makers.
[0:24:34 - 0:24:39]  It's being made about 200 different skews, 200 different configurations.
[0:24:40 - 0:24:49]  They're liquid-cooled, air-cooled, x86, and video-gray CPU versions, NVLink 36 by 2 NVLink 702 by 1.
[0:24:49 - 0:24:54]  And a lot of different types of systems so that we can accommodate just about every single data center in the world.
[0:24:55 - 0:25:01]  Well, these systems are being currently manufactured in some 45 factories.
[0:25:01 - 0:25:10]  It tells you how pervasive artificial intelligence is and how much the industry is jumping on to artificial intelligence in this new computing model.
[0:25:12 - 0:25:17]  Well, the reason why we're driving is so hard is because we need a lot more computation.
[0:25:17 - 0:25:24]  It's very clear that Janine.
[0:25:35 - 0:25:41]  It's hard to tell everyone to reach your hands into a dark place.
[0:25:43 - 0:25:44]  Is this a good idea?
[0:25:44 - 0:25:47]  Alright.
[0:26:03 - 0:26:05]  Wait for it.
[0:26:06 - 0:26:08]  Wait for it.
[0:26:12 - 0:26:13]  I thought I was worthy.
[0:26:18 - 0:26:20]  Apparently, Janine didn't think I was worthy.
[0:26:22 - 0:26:25]  Alright. This is my show-and-tell.
[0:26:27 - 0:26:36]  So, this NVLink system, this right here, this NVLink system, this is GB200 NVLink 72.
[0:26:37 - 0:26:50]  It is 1.5 tons, 600,000 parts, approximately equal to 20 cars, 12 120 kilowatts.
[0:26:53 - 0:27:05]  It has a spine behind it that connects all of these GPU together, two miles of copper cable, 5,000 cables,
[0:27:07 - 0:27:10]  this is being manufactured in 45 factories around the world.
[0:27:11 - 0:27:21]  We build them, we liquid cool them, we test them, we disassemble them, ship them parts to the data centers because it's 1.5 tons.
[0:27:22 - 0:27:25]  We reassemble it outside the data centers and install them.
[0:27:25 - 0:27:27]  The manufacturing is insane.
[0:27:27 - 0:27:34]  But the goal of all of this is because the scaling laws are driving computing so hard that this level of computation,
[0:27:34 - 0:27:41]  Blackwell over our last generation, improves the performance per watt by a factor of 4.
[0:27:42 - 0:27:47]  Performance per watt by a factor of 4, performance per dollar by a factor of 3.
[0:27:47 - 0:27:56]  That basically says that in one generation, we reduce the cost of training these models by a factor of 3.
[0:27:56 - 0:28:01]  Or if you want to increase the size of your model by a factor of 3, it's about the same cost.
[0:28:01 - 0:28:10]  But the important thing is this, these are generating tokens that are being used by all of us when we use Chatchy PT or when we use Gemini,
[0:28:10 - 0:28:15]  use our phones in the future, just by all of these applications going to be consuming these AI tokens.
[0:28:16 - 0:28:18]  And these AI tokens are being generated by these systems.
[0:28:19 - 0:28:22]  And every single data center is limited by power.
[0:28:23 - 0:28:33]  And so if the per watt of Blackwell is 4 times our last generation, then the revenue that could be generated,
[0:28:34 - 0:28:37]  the amount of business that could be generated in the data center is increased by a factor of 4.
[0:28:38 - 0:28:42]  And so these AI factory systems really are factories today.
[0:28:42 - 0:28:47]  Now the goal of all of this is to so that we can create one giant chip.
[0:28:47 - 0:28:50]  The amount of computation we need is really quite incredible.
[0:28:51 - 0:28:53]  And this is basically one giant chip.
[0:28:53 - 0:28:56]  If we were to have to build a chip, one, here we go.
[0:28:57 - 0:28:57]  Sorry you guys.
[0:28:59 - 0:29:00]  You see that? That's cool.
[0:29:02 - 0:29:03]  Look at that. Disco lights in here.
[0:29:07 - 0:29:11]  If we had to build this as one chip, obviously this would be the size of the wafer,
[0:29:11 - 0:29:14]  but this doesn't include the impact of yield.
[0:29:14 - 0:29:16]  It would have to be probably three or four times the size.
[0:29:17 - 0:29:21]  But what we basically have here is 72 Blackwell GPUs are 144 dies.
[0:29:22 - 0:29:26]  This one chip here is 1.4 exa flops.
[0:29:27 - 0:29:30]  The world's largest supercomputer, fastest supercomputer, only recently,
[0:29:30 - 0:29:35]  this entire room supercomputer only recently achieved an exa flop plus.
[0:29:36 - 0:29:40]  This is 1.4 exa flops of AI floating point performance.
[0:29:40 - 0:29:43]  It has 14 terabytes of memory, but here's the amazing thing.
[0:29:44 - 0:29:47]  The memory bandwidth is 1.2 petabytes per second.
[0:29:48 - 0:29:55]  That's basically the entire internet traffic that's happening right now.
[0:29:56 - 0:30:02]  The entire world's internet traffic is being processed across these chips.
[0:30:03 - 0:30:08]  We have 130 trillion transistors in total,
[0:30:09 - 0:30:14]  2,592 CPU cores, a whole bunch of networking.
[0:30:15 - 0:30:17]  I wish I could do this.
[0:30:17 - 0:30:18]  I don't think I will.
[0:30:18 - 0:30:19]  These are the Blackwells.
[0:30:21 - 0:30:26]  These are our ConnectX networking chips.
[0:30:26 - 0:30:32]  These are the NV link, and we're trying to pretend about the NV link spine,
[0:30:32 - 0:30:33]  but that's not possible.
[0:30:35 - 0:30:40]  These are all of the HBM memories, 12-14 terabytes of HBM memory.
[0:30:41 - 0:30:45]  This is what we're trying to do, and this is the miracle of the Blackwell system.
[0:30:45 - 0:30:47]  The Blackwell dies right here.
[0:30:48 - 0:30:50]  It is the largest single chip the world's ever made.
[0:30:51 - 0:30:55]  But yet, the miracle is really in addition to that.
[0:30:56 - 0:30:57]  This is the Grace Blackwell system.
[0:30:58 - 0:31:01]  Well, the goal of all of this, of course, is so that we can...
[0:31:01 - 0:31:02]  Thank you, thanks.
[0:31:06 - 0:31:08]  Boy, is there a chair I could sit down for a second?
[0:31:19 - 0:31:20]  Okay.
[0:31:21 - 0:31:23]  Can I have a Mikalobe Ultra?
[0:31:34 - 0:31:40]  How is it possible that we're in the Mikalobe Ultra Stadium?
[0:31:42 - 0:31:45]  It's like coming to an Nvidia, and we don't have a GPU for you.
[0:31:50 - 0:31:55]  So, we need an enormous amount of computation, because we want to train larger and larger models,
[0:31:56 - 0:32:01]  and these inferences used to be one inference, but in the future,
[0:32:01 - 0:32:02]  the AI is going to be talking to itself.
[0:32:03 - 0:32:03]  It's going to be thinking.
[0:32:04 - 0:32:06]  It's going to be internally reflecting, processing.
[0:32:07 - 0:32:10]  So, today, when the tokens are being generated at you,
[0:32:10 - 0:32:15]  so long as it's coming out at 20 or 30 tokens per second,
[0:32:15 - 0:32:17]  it's basically as fast as anybody can read.
[0:32:17 - 0:32:21]  However, in the future, and right now with GPT-01,
[0:32:22 - 0:32:27]  with the new Gemini Pro and the new G-01-03 models,
[0:32:28 - 0:32:31]  they're talking to themselves, reflecting, they're thinking.
[0:32:32 - 0:32:37]  And so, as you can imagine, the rate at which the tokens could be ingested is incredibly high.
[0:32:37 - 0:32:41]  And so, we need the token rates, the token generation rates, to go way up.
[0:32:42 - 0:32:45]  And we also have to drive the costs way down simultaneously,
[0:32:45 - 0:32:48]  so that the quality of service can be extraordinary.
[0:32:49 - 0:32:53]  The cost to customers can continue to be low, and AI will continue to scale.
[0:32:53 - 0:32:57]  And so, that's the fundamental purpose, the reason why we created an MB link.
[0:32:57 - 0:33:02]  Well, one of the most important things that's happening in the world of enterprise is agentic AI.
[0:33:03 - 0:33:07]  Agentic AI, basically, is a perfect example of test timescaling.
[0:33:07 - 0:33:09]  It's a system of models.
[0:33:10 - 0:33:14]  Some of it is understanding, interacting with the customer, interacting with the user.
[0:33:14 - 0:33:19]  Some of it's maybe retrieving information, retrieving information from storage,
[0:33:19 - 0:33:21]  a semantic AI system, like a rag.
[0:33:22 - 0:33:24]  Maybe it's going on to the internet.
[0:33:25 - 0:33:27]  Maybe it's studying a PDF file.
[0:33:28 - 0:33:29]  And so, it might be using tools.
[0:33:29 - 0:33:30]  It might be using a calculator.
[0:33:31 - 0:33:35]  And it might be using a generative AI to generate charts and such.
[0:33:35 - 0:33:39]  And it's taking the problem you gave it, breaking it down step by step,
[0:33:40 - 0:33:41]  and it's iterating through all these different models.
[0:33:42 - 0:33:46]  Well, in order to respond to a customer in the future, in order for AI to respond,
[0:33:46 - 0:33:50]  it used to be ask a question, answer starts spewing out, in the future,
[0:33:50 - 0:33:54]  you ask a question, a whole bunch of models are going to be working in the background.
[0:33:54 - 0:34:00]  And so, test timescaling, the amount of computation used for inferencing,
[0:34:00 - 0:34:01]  is going to go through the roof.
[0:34:02 - 0:34:04]  It's going to go through the roof because we want better and better answers.
[0:34:05 - 0:34:12]  Well, to help the industry build a gentick AI, our go-to-market is not direct to enterprise customers.
[0:34:12 - 0:34:17]  Our go-to-market is we work with software developers in the IT ecosystem
[0:34:17 - 0:34:21]  to integrate our technology to make possible new capabilities,
[0:34:21 - 0:34:27]  just like we did with CUDA libraries, we now want to do that with AI libraries.
[0:34:28 - 0:34:34]  And just as the computing model of the past has APIs that are doing computer graphics
[0:34:34 - 0:34:38]  or doing linear algebra or doing fluid dynamics, in the future,
[0:34:38 - 0:34:42]  on top of those acceleration libraries, CUDA acceleration libraries,
[0:34:42 - 0:34:44]  we'll have AI libraries.
[0:34:45 - 0:34:49]  We've created three things for helping the ecosystem build a gentick AI.
[0:34:49 - 0:34:55]  And VDNims, which are essentially AI microservices all packaged up.
[0:34:55 - 0:34:59]  It takes all of this really complicated CUDA software, CUDA DNN,
[0:35:00 - 0:35:07]  Cutlass or TensorFlow, or Triton, or all of these different really complicated software.
[0:35:07 - 0:35:11]  And the model itself, we package it up, we optimize it, we put it into a container,
[0:35:12 - 0:35:13]  and you could take it wherever you like.
[0:35:14 - 0:35:18]  And so, we have models for vision, for understanding languages, for speech,
[0:35:18 - 0:35:24]  for animation, for digital biology, and we have some new exciting models coming for physical AI.
[0:35:25 - 0:35:28]  And these AI models run in every single cloud,
[0:35:28 - 0:35:31]  because NVIDIA's GPUs are now available in every single cloud,
[0:35:31 - 0:35:34]  it's available in every single OEM, so you could literally take these models,
[0:35:35 - 0:35:40]  integrate it into your software packages, create AI agents that run on cadence,
[0:35:41 - 0:35:46]  or there might be service now agents, or they might be SAP agents,
[0:35:46 - 0:35:50]  and they could deploy it to their customers and run it wherever the customers want to run this software.
[0:35:51 - 0:35:53]  The next layer is what we call NVIDIA Nemo.
[0:35:54 - 0:36:04]  Nemo is essentially a digital employee onboarding and training evaluation system.
[0:36:05 - 0:36:12]  In the future, these AI agents are essentially digital workforce that are working alongside your employees,
[0:36:12 - 0:36:16]  working on doing things for you on your behalf.
[0:36:16 - 0:36:20]  And so, the way that you would bring these specialized agents into your,
[0:36:21 - 0:36:26]  the special agents, into your company is to onboard them, just like you onboard an employee.
[0:36:27 - 0:36:35]  And so, we have different libraries that helps these AI agents be trained for the type of language in your company,
[0:36:35 - 0:36:39]  maybe the vocabularies, unique to your company, the business process is different,
[0:36:39 - 0:36:44]  the way you work is different, so you would give them examples of what the work products you look like,
[0:36:45 - 0:36:50]  and they would try to generate it, and you would give a feedback, and then you would evaluate them, so on and so forth.
[0:36:50 - 0:36:54]  And so, that, and you would guard rail them, you say, these are the things that you're not allowed to do,
[0:36:54 - 0:36:59]  these are the things that you're not allowed to say, and we even give them access to certain information.
[0:37:00 - 0:37:06]  So, that entire pipeline, a digital employee pipeline, is called Nemo.
[0:37:06 - 0:37:14]  In a lot of ways, the IT department of every company is going to be the HR department of AI agents in the future.
[0:37:15 - 0:37:21]  Today, they manage and maintain a bunch of software from the IT industry,
[0:37:21 - 0:37:28]  and the future that will maintain, you know, nurture onboard, and improve a whole bunch of digital agents,
[0:37:28 - 0:37:30]  and provision them to the companies to use.
[0:37:30 - 0:37:35]  Okay, and so your IT department is going to become kind of like AI agent HR.
[0:37:36 - 0:37:43]  And on top of that, we provide a whole bunch of blueprints that our ecosystem could take advantage of.
[0:37:43 - 0:37:48]  All of this is completely open source, and so you could take it and modify the blueprints,
[0:37:48 - 0:37:51]  we have blueprints for all kinds of different types of agents.
[0:37:51 - 0:37:56]  Well, today, we're also announcing that we're doing something that's really cool, and I think really clever.
[0:37:56 - 0:38:06]  We're announcing a whole family of models that are based off of Lama, the Envidia Lama Nemo Tron Language Foundation models.
[0:38:07 - 0:38:11]  Lama 3.1 is a complete phenomenon.
[0:38:12 - 0:38:18]  The download of Lama 3.1 from Meta, 350, 650,000 times, something like that.
[0:38:18 - 0:38:27]  It has been derived and turned into other models, about 60,000 other different models.
[0:38:28 - 0:38:34]  It is singularly the reason why just about every single enterprise and every single industry has been activated to start working on AI.
[0:38:35 - 0:38:42]  Well, the thing that we did was we realized that the Lama models really could be better fine-tuned for enterprise use.
[0:38:42 - 0:38:51]  So we've fine-tuned them using our expertise and our capabilities, and we turn them into the Lama Nemo Tron suite of open models.
[0:38:52 - 0:38:58]  There are small ones that interact in very fast response time, extremely small.
[0:38:59 - 0:39:03]  They're what we call super Lama Nemo Tron supers.
[0:39:03 - 0:39:08]  They're basically your mainstream versions of your models, or your ultra model.
[0:39:08 - 0:39:13]  The ultra model could be used to be a teacher model for a whole bunch of other models.
[0:39:13 - 0:39:22]  It could be a reward model, evaluator, a judge for other models to create answers and decide whether it's a good answer or not.
[0:39:22 - 0:39:24]  Basically give feedback to other models.
[0:39:24 - 0:39:26]  It could be distilled in a lot of different ways.
[0:39:26 - 0:39:32]  Basically a teacher model, a knowledge distillation model, very large, very capable.
[0:39:33 - 0:39:35]  And so all of this is now available online.
[0:39:35 - 0:39:39]  Well, these models are incredible.
[0:39:40 - 0:39:48]  It's a number one in leaderboards for chat, leaderboard for instruction, leaderboard for retrieval.
[0:39:49 - 0:39:56]  So the different types of functionalities necessary that are used in AI agents around the world, these are going to be incredible models for you.
[0:39:57 - 0:40:00]  We're also working with the ecosystem.
[0:40:00 - 0:40:06]  These all of our NVIDIA AI technologies are integrated into the IT industry.
[0:40:07 - 0:40:14]  We have great partners and really great work being done at ServiceNow, at SAP, at Siemens, for industrial AI.
[0:40:15 - 0:40:18]  Cadence is doing great works and offices doing great work.
[0:40:18 - 0:40:20]  I'm really proud of the work that we do with perplexity.
[0:40:20 - 0:40:22]  As you know, they revolutionize search.
[0:40:22 - 0:40:24]  Yeah, really fantastic stuff.
[0:40:25 - 0:40:28]  A code-ium, every software engineer in the world.
[0:40:28 - 0:40:32]  This is going to be the next giant AI application.
[0:40:32 - 0:40:37]  Next giant AI service period is software coding.
[0:40:37 - 0:40:39]  30 million software engineers around the world.
[0:40:40 - 0:40:44]  Everybody is going to have a software assistant helping them code.
[0:40:44 - 0:40:50]  If not, obviously you're just going to be way less productive and create lesser good code.
[0:40:50 - 0:40:52]  And so this is 30 million.
[0:40:53 - 0:40:55]  There's a billion knowledge workers in the world.
[0:40:56 - 0:40:57]  It is very, very clear.
[0:40:57 - 0:41:04]  AI agents is probably the next robotics industry and likely to be a multi-trillion dollar opportunity.
[0:41:04 - 0:41:12]  Well, let me show you some of the blueprints that we created and some of the work that we've done with our partners with these AI agents.
[0:41:17 - 0:41:22]  AI agents are the new digital workforce working for and with us.
[0:41:24 - 0:41:34]  AI agents are a system of models that reason about a mission, break it down into tasks, and retrieve data or use tools to generate a quality response.
[0:41:35 - 0:41:46]  In video's agentic AI building blocks, NIM pre-trained models and NIMO framework let organizations easily develop AI agents and deploy them anywhere.
[0:41:46 - 0:41:53]  We will onboard and train our agentic workforces on our company's methods like we do for employees.
[0:41:54 - 0:41:57]  AI agents are domain-specific task experts.
[0:41:58 - 0:41:59]  Let me show you for examples.
[0:42:00 - 0:42:14]  For the billions of knowledge workers and students, AI research assistant agents ingest complex documents like lectures, journals, financial results, and generate interactive podcasts for easy learning.
[0:42:14 - 0:42:23]  By combining a UNET regression model with a diffusion model, CoreDeF can downscale global weather forecasts down from 25 kilometers to 2 kilometers.
[0:42:24 - 0:42:35]  Developers, like at NVIDIA, manage software security AI agents that continuously scans software for vulnerabilities, alerting developers to what action is needed.
[0:42:37 - 0:42:46]  Virtual lab AI agents help researchers design and screen billions of compounds to find promising drug candidates faster than ever.
[0:42:47 - 0:43:00]  And video analytics AI agents built on an NVIDIA metropolis blueprint, including NVIDIA Cosmos NIMATron Vision Language Models, Lama NIMATron LLMs, and NIMO Retriever.
[0:43:01 - 0:43:09]  Metropolis agents analyze content from the billions of cameras generating 100,000 petabytes of video per day.
[0:43:09 - 0:43:14]  They enable interactive search, summarization, and automated reporting.
[0:43:15 - 0:43:20]  And help monitor traffic flows, flagging congestion or danger.
[0:43:22 - 0:43:29]  In industrial facilities, they monitor processes and generate recommendations or improvement.
[0:43:30 - 0:43:38]  Metropolis agents centralize data from hundreds of cameras and can reroute workers or robots when incidents occur.
[0:43:39 - 0:43:44]  The age of a genetic AI is here for every organization.
[0:43:47 - 0:43:48]  Okay.
[0:43:52 - 0:43:59]  That was the first pitch at a baseball game. That was not generated. I just felt that none of you were impressed.
[0:44:00 - 0:44:06]  Okay. So AI was created in the cloud and for the cloud.
[0:44:07 - 0:44:15]  AI is creating the cloud for the cloud. And for enjoying AI on phones, of course, it's perfect. Very, very soon.
[0:44:15 - 0:44:26]  We're going to have a continuous AI that's going to be with you. And when you use those metaglasses, you could, of course, point at something, look at something and ask it, you know, whatever information you want.
[0:44:26 - 0:44:31]  And so AI is perfect in the cloud. It was created in the cloud. It was perfect in the cloud.
[0:44:32 - 0:44:40]  However, we would love to be able to take that AI everywhere. I've mentioned already that you could take and video AI to any cloud, but you could also put it inside your company.
[0:44:41 - 0:44:44]  But the thing that we want to do more than anything is put it on our PC as well.
[0:44:45 - 0:44:56]  And so, as you know, Windows 95 revolutionized the computer industry. It made possible this new suite of multimedia services and then changed the way that applications was created forever.
[0:44:57 - 0:45:03]  Windows 95, this model of computing, of course, is not perfect for AI.
[0:45:04 - 0:45:10]  And so the thing that we would like to do is we would like to have in the future your AI basically become your AI assistant.
[0:45:11 - 0:45:24]  And instead of just the 3D APIs and the sound APIs and the video APIs, you would have generative APIs, generative APIs for 3D and generative APIs for language and generative AI for sound and so on and so forth.
[0:45:24 - 0:45:32]  And we need a system that makes that possible while leveraging the massive investment that's in the cloud.
[0:45:33 - 0:45:39]  There's no way that we could the world can create yet another way of programming AI models. It's just not going to happen.
[0:45:40 - 0:45:50]  And so if we could figure out a way to make Windows PC, a world class AI PC, it would be completely awesome.
[0:45:50 - 0:45:57]  And it turns out the answer is Windows. It's Windows WSL 2. Windows WSL 2.
[0:45:57 - 0:46:09]  Windows WSL 2 basically is two operating systems within one. It works perfectly. It's developed for developers and it's developed so that you can have access to bare metal.
[0:46:09 - 0:46:20]  So it's been WSL 2 has been optimized for cloud native applications. It is optimized for and very importantly, it's been optimized for CUDA.
[0:46:21 - 0:46:26]  And so WSL 2 supports CUDA perfectly out of the box as a result.
[0:46:26 - 0:46:43]  So everything that I showed you with NVIDIA NIMS, NVIDIA Nemo, the Blueprints that we developed that are going to be up in ai.nVIDIA.com, so long as the computer fits it.
[0:46:43 - 0:46:58]  So long as you can fit that model and we're going to have many models that fit, whether it's vision models or language models or speech models or these animation, human digital human models, all kinds of different types of models are going to be perfect for your PC.
[0:46:58 - 0:47:15]  So we're going to have a PC and it would download it and it should just run. And so our focus is to turn Windows WSL 2. Windows PC into a target first class platform that we will support and maintain for as long as we shall live.
[0:47:16 - 0:47:24]  And so this is an incredible thing for engineers and developers everywhere. Let me show you something that we can do with that. This is one of the examples of a blueprint we just made for you.
[0:47:27 - 0:47:36]  Generative AI synthesizes amazing images from simple text prompts. Yet image composition can be challenging to control using only words.
[0:47:37 - 0:47:44]  With NVIDIA NIM microservices, creators can use simple 3D objects to guide AI image generation.
[0:47:44 - 0:47:55]  Let's see how a concept artist can use this technology to develop the look of a scene. They start by laying out 3D assets created by hand or generated with AI.
[0:47:56 - 0:48:02]  Then use an image generation NIM such as flux to create a visual that adheres to the 3D scene.
[0:48:03 - 0:48:16]  Add or move objects to refine the composition. Change camera angles to frame the perfect shot or reimagine the whole scene with a new prompt.
[0:48:19 - 0:48:25]  Assisted by generative AI and NVIDIA NIM, an artist can quickly realize their vision.
[0:48:28 - 0:48:30]  NVIDIA AI for your PCs.
[0:48:33 - 0:48:39]  Hundreds of millions of PCs in the world with windows. And so we could get them ready for AI.
[0:48:39 - 0:48:46]  OEMs, all the PCOEMs we work with, just basically all the world's leading PCOEMs are going to get their PCs ready for this stack.
[0:48:47 - 0:48:50]  And so AI PCs are coming to a home near you.
[0:48:56 - 0:48:57]  The next is good.
[0:49:03 - 0:49:05]  Okay, let's talk about physical AI.
[0:49:07 - 0:49:10]  Speaking of Linux, let's talk about physical AI.
[0:49:12 - 0:49:13]  So, physical AI.
[0:49:14 - 0:49:32]  Imagine, imagine, whereas your large language model, you give it your context, your prompt, on the left, and it generates tokens one at a time to produce the output.
[0:49:33 - 0:49:34]  That's basically how it works.
[0:49:34 - 0:49:39]  The amazing thing is, this model in the middle is quite large, has billions of parameters.
[0:49:40 - 0:49:50]  The context length is incredibly large because you might decide to load in a PDF, in my case, I might load in several PDFs before I ask you a question.
[0:49:51 - 0:50:02]  Those PDFs are turned into tokens. The basic attention characteristic of a transformer has every single token find its relationship and relevance against every other token.
[0:50:02 - 0:50:19]  So, you could have hundreds of thousands of tokens, and the computational load increases quadratically, and it does this, all of the parameters, all of the input sequence, process it through every single layer of the transformer, and it produces one token.
[0:50:19 - 0:50:21]  That's the reason why we needed blackwap.
[0:50:21 - 0:50:33]  And then the next token is produced. When the current token is done, it puts the current token into the input sequence, and takes that whole thing, and generates the next token, it doesn't one at a time.
[0:50:34 - 0:50:41]  This is the transformer model. It's the reason why it is so incredibly effective, computationally demanding.
[0:50:41 - 0:50:53]  What if instead of PDFs, it's your surrounding, and what if instead of the prompt, a question, it's a request. Go over there and pick up that box and bring it back.
[0:50:54 - 0:50:59]  And instead of what is produced in tokens, it's text, it produces action tokens.
[0:51:00 - 0:51:09]  Well, that I just described is a very sensible thing for the future of robotics, and the technology is right around the corner.
[0:51:09 - 0:51:26]  But what we need to do is we need to create the effectively, the world model, as opposed to GBT, which is a language model, and this world model has to understand the language of the world, has to understand physical dynamics.
[0:51:27 - 0:51:36]  Things like gravity and friction and inertia, it has to understand geometric and spatial relationships, and it has to understand cause and effect.
[0:51:36 - 0:51:43]  If you drop something and floss it a ground, you poke at it and it tips over, and I still understand object permanence.
[0:51:44 - 0:51:51]  If you roll a ball over the kitchen counter, when it goes off the other side, the ball didn't leave into another quantum universe that's still there.
[0:51:52 - 0:52:00]  And so all of these types of understanding us, intuitive understanding that we know that most models today have a very hard time with.
[0:52:00 - 0:52:13]  And so we would like to create a world, we need a world foundation model.
[0:52:13 - 0:52:21]  That is designed, that was created to understand the physical world, and the only way for you to really understand this is to see it. Let's play it.
[0:52:28 - 0:52:31]  The next frontier of AI is physical AI.
[0:52:32 - 0:52:40]  Model performance is directly related to data availability, but physical world data is costly to capture, curate, and label.
[0:52:42 - 0:52:48]  Envidia Cosmos is a world foundation model development platform to advance physical AI.
[0:52:48 - 0:53:00]  It includes auto regressive world foundation models, diffusion based world foundation models, advanced tokenizers, and an Envidia CUDA, an AI accelerated data pipeline.
[0:53:03 - 0:53:09]  Cosmos models in just text, image, or video prompts, and generate virtual world states as videos.
[0:53:10 - 0:53:20]  Cosmos generations prioritize the unique requirements of AV and robotics use cases, like real world environments, lighting, and object permits.
[0:53:22 - 0:53:35]  Developers use Envidia Omniverse to build physics-based, geospatially accurate scenarios, then output omniverse renders into Cosmos, which generates photo-real, physically-based, synthetic data.
[0:53:46 - 0:54:07]  Whether diverse objects, or environments, conditions like weather, or time of day, or edge-case scenarios, developers use Cosmos to generate worlds for reinforcement learning AI feedback to improve policy models,
[0:54:08 - 0:54:15]  or to test and validate model performance, even across multi-sensor views.
[0:54:17 - 0:54:30]  Cosmos can generate tokens in real time, bringing the power of four-cylinder and multiverse simulation to AI models, generating every possible future to help the models select the right path.
[0:54:31 - 0:54:38]  Working with the world's developer ecosystem, Envidia is helping advance the next wave of physical AI.
[0:54:43 - 0:54:54]  Envidia Cosmos. Envidia Cosmos. Envidia Cosmos. The world's first world foundation model.
[0:54:55 - 0:55:05]  It is trained on 20 million hours of video. The 20 million hours of video focuses on physical dynamic things.
[0:55:05 - 0:55:18]  So dynamic nature themes, themes, humans, walking, hands moving, manipulating things, you know, things that are fast camera movements.
[0:55:18 - 0:55:26]  It's really about teaching the AI, not about generating creative content, but teaching the AI to understand the physical world.
[0:55:27 - 0:55:34]  And with this physical AI, there are many downstream things that we could do as a result.
[0:55:34 - 0:55:43]  We could do synthetic data generation to train models. We could distill it and turn it into effectively the seed, the beginnings of a robotics model.
[0:55:43 - 0:55:52]  You could have it generate multiple physically based, physically plausible scenarios of the future, basically do a doctor strange.
[0:55:54 - 0:55:59]  Because this model understands the physical world, of course, you saw a whole bunch of images generated.
[0:55:59 - 0:56:04]  This model understanding the physical world, it also could do, of course, captioning.
[0:56:04 - 0:56:18]  And so it could take videos, caption it incredibly well, and that captioning and the video could be used to train large language models, multi-modality large language models.
[0:56:19 - 0:56:26]  And so you could use this technology to use this foundation model to train robotics, robots as well as large language models.
[0:56:26 - 0:56:36]  And so this is the NVIDIA Cosmos. The platform has an auto regressive model for real time applications, as diffusion model for a very high quality image generation.
[0:56:36 - 0:56:53]  It's an incredible tokenizer, basically learning the vocabulary of real world and a data pipeline so that if you would like to take all of this and then train it on your own data, this data pipeline because there's so much data involved, we've accelerated everything end to end for you.
[0:56:53 - 0:57:02]  And so this is the world's first data processing pipeline that could accelerate it as well as AI accelerated. All of this is part of the Cosmos platform.
[0:57:02 - 0:57:08]  And today we're announcing that Cosmos is open licensed. It's open available on GitHub.
[0:57:14 - 0:57:28]  We hope that this moment, and there's a small medium large for very fast models, mainstream models, and also teacher models, basically not knowledge transfer models.
[0:57:28 - 0:57:39]  Cosmos world foundation model being open, we really hope, we'll do for the world of robotics and industrial AI what Lama 3 has done for enterprise AI.
[0:57:41 - 0:57:47]  The magic happens when you connect Cosmos to omniverse and the reason fundamentally is this.
[0:57:47 - 0:58:02]  Omniverse is a physics grounded, not physically grounded, but physics grounded. It's algorithmic physics, principled physics simulation grounded system. It's a simulator.
[0:58:02 - 0:58:13]  When you connect that to Cosmos, it provides the grounding, the ground truth that can control into condition the Osmos generation.
[0:58:14 - 0:58:25]  As a result what comes out of Osmos is grounded on truth. This is exactly the same idea as connecting a large language model to a reg to a retrieval augmented generation system.
[0:58:25 - 0:58:39]  You want to ground the AI generation on ground truth. And so the combination of the two gives you a physically simulated, a physically grounded multiverse generator.
[0:58:39 - 0:58:49]  And the application, the use cases are really quite exciting. And of course for robotics, for industrial applications, it is very, very clear.
[0:58:49 - 0:58:59]  This Cosmos plus omniverse plus Cosmos represents the third computer that's necessary for building robotic systems.
[0:58:59 - 0:59:08]  Every robotics company will ultimately have to build three computers. A robotic system could be a factory, a robotic system could be a car, it could be a robot.
[0:59:08 - 0:59:16]  You need three fundamental computers. One computer of course, to train the AI. We call it the DGX computer to train the AI.
[0:59:17 - 0:59:29]  Another, of course, when you're done, to deploy the AI, we call that AGX. That's inside the car in the robot or in an AMR or in a stadium or whatever it is.
[0:59:29 - 0:59:38]  These computers are at the edge and they're autonomous. But to connect the two, you need a digital twin. And this is all the simulations that you were seeing.
[0:59:38 - 0:59:51]  The digital twin is where the AI that has been trained goes to practice, to be refined, to do its synthetic data generation, reinforcement learning AI feedback, such and such.
[0:59:51 - 1:00:04]  And so it's the digital twin of the AI. These three computers are going to be working interactively. And Vitya's strategy for the industrial world, and we've been talking about this for some time, is this three computer system.
[1:00:05 - 1:00:12]  You know, instead of a three body problem, we have a three computer solution. And so, it's the Nvidia robotics.
[1:00:17 - 1:00:29]  So let me give you three examples. All right. So the first example is how we apply all of this to industrial visualization.
[1:00:29 - 1:00:42]  There are millions of factories, hundreds of thousands of warehouses. That's basically the backbone of a $50 trillion manufacturing industry. All of that has to become software defined.
[1:00:42 - 1:00:48]  All of it has to have automation in the future. And all of it will be infused with robotics.
[1:00:48 - 1:01:04]  And we're partnering with Keon, the world's leading warehouse automation solutions provider, and Accenture, the world's largest professional services provider, and they have a big focus in digital manufacturing.
[1:01:04 - 1:01:09]  And we're working together to create something that's really special. Now I'll show you that in a second.
[1:01:09 - 1:01:21]  But our go-to-market is essentially the same as all of the other software platforms and all the technology platforms that we have through the developers and ecosystem partners.
[1:01:21 - 1:01:29]  And we have just a growing number of ecosystem partners connecting to omniverse. And the reason for that is very clear.
[1:01:29 - 1:01:39]  Everybody wants to visualize the future of industries. There's so much waste, so much opportunity for automation in that $50 trillion of the world's GDP.
[1:01:40 - 1:01:45]  So let's take a look at that. This one one one one example that we're doing with Keon and Accenture.
[1:01:47 - 1:02:02]  Keon, the supply chain solution company Accenture, a global leader in professional services, and Nvidia are bringing physical AI to the $1 trillion warehouse and distribution center market.
[1:02:03 - 1:02:12]  Managing high performance warehouse logistics involves navigating a complex web of decisions influenced by constantly shifting variables.
[1:02:12 - 1:02:23]  These include daily and seasonal demand changes, space constraints, workforce availability, and the integration of diverse robotic and automated systems.
[1:02:23 - 1:02:29]  And predicting operational KPIs of a physical warehouse is nearly impossible today.
[1:02:30 - 1:02:41]  To tackle these challenges, Keon is adopting mega and Nvidia omniverse blueprint for building industrial digital twins to test and optimize robotic fleets.
[1:02:41 - 1:02:53]  First, Keon's warehouse management solution assigns tasks to the industrial AI brains in the digital twin such as moving a load from a buffer location to a shuttle storage solution.
[1:02:54 - 1:03:10]  The robots brains are in a simulation of a physical warehouse, digitalized into omniverse using open USD connectors to aggregate CAD, video and image to 3D, LiDAR to point cloud, and AI generated data.
[1:03:10 - 1:03:20]  The fleet of robots execute tasks by perceiving and reasoning about their omniverse digital twin environment, planning their next motion and acting.
[1:03:20 - 1:03:27]  The robot brains can see the resulting state through sensor simulations and decide their next action.
[1:03:27 - 1:03:33]  The loop continues while mega precisely tracks the state of everything in the digital twin.
[1:03:34 - 1:03:48]  Now, Keon can simulate infinite scenarios at scale while measuring operational KPIs such as throughput, efficiency, and utilization all before deploying changes to the physical warehouse.
[1:03:48 - 1:03:54]  Together with Nvidia, Keon and Accenture are reinventing industrial autonomy.
[1:03:57 - 1:03:59]  The future is not that incredible, everything is in simulation.
[1:04:00 - 1:04:10]  In the future, in the future, every factory will have a digital twin, and that digital twin operates exactly like the real factory.
[1:04:10 - 1:04:30]  In fact, you could use omniverse with cosmos to generate a whole bunch of future scenarios, and you pick then an AI decides which one of the scenarios are the most optimal for whatever KPIs, and that becomes the programming constraints, the program, if you will, the AI's that will be deployed into the real factories.
[1:04:30 - 1:04:44]  The next example, autonomous vehicles. The AV revolution has arrived. After so many years, with way more success and Tesla's success, it is very, very clear autonomous vehicles has finally arrived.
[1:04:50 - 1:04:59]  The next example, the AI's, the simulation systems, and the synthetic data generation systems on the universe, and now cosmos, and also the computer that is inside the car.
[1:05:00 - 1:05:05]  Each car company might work with us in a different way, use one or two or three of the computers.
[1:05:05 - 1:05:23]  We're working with just about every major car company around the world, Waymo and Zooks, and Tesla, of course, in their data center, BYD, the largest EV company in the world, JLRs, got a really cool car coming, Mercedes, because a fleet of cars coming with Envidia starting this year going to production.
[1:05:24 - 1:05:32]  And I'm super, super pleased to announce that today, Toyota, and Envidia are going to partner together to create their next generation AVs.
[1:05:39 - 1:05:48]  Just so many, so many cool companies, Lucid and Rivian and Xiaomi, and, of course, Volvo, just so many different companies.
[1:05:48 - 1:05:56]  Wabi is building self-driving trucks, Aurora, we announced this week also that Aurora is going to use Envidia to build self-driving trucks.
[1:05:57 - 1:06:07]  Autonomous, 100 million cars build each year, a billion cars vehicles on a road all over the world, a trillion miles that are driven around the world each year.
[1:06:08 - 1:06:13]  That's all going to be either highly autonomous or, you know, fully autonomous coming up.
[1:06:13 - 1:06:22]  And so this is going to be a very large industry. I predict that this will likely be the first multi-trillion dollar robotics industry.
[1:06:22 - 1:06:35]  This business for us, notice in just a few of these cars that are starting to ramp into the world, our business is already $4 billion, and this year probably on a run rate about $5 billion.
[1:06:36 - 1:06:38]  So really significant business already, this is going to be very large.
[1:06:38 - 1:06:46]  Well, today we're announcing that our next generation processor for the car. Our next generation computer for the car is called Thor.
[1:06:47 - 1:06:48]  I have one right here, hang on a second.
[1:06:51 - 1:06:55]  Okay, this is Thor. This is Thor.
[1:06:56 - 1:07:00]  This is a robotics computer.
[1:07:01 - 1:07:07]  This is a robotics computer, takes sensors, and just a madness amount of sensor information.
[1:07:07 - 1:07:17]  Process it, you know, umpteen cameras, high resolution, radars, light hours, they're all coming into this chip.
[1:07:18 - 1:07:25]  And this chip has to process all that sensor, turn them into tokens, put them into a transformer, and predict the next path.
[1:07:26 - 1:07:36]  And this AV computer is now in full production. Thor is 20 times the processing capability of our last generation, Orin,
[1:07:36 - 1:07:43]  which is really the standard of autonomous vehicles today. And so this is just really quite incredible. Thor is in full production.
[1:07:44 - 1:07:53]  This robotics processor, by the way, also goes into a full robot. And so it could be an AMR, it could be a human robot, it could be the brain, it could be the manipulator.
[1:07:54 - 1:07:58]  This processor basically is a universal robotics computer.
[1:07:59 - 1:08:06]  The second part of our drive system that I'm incredibly proud of is the dedication to safety.
[1:08:07 - 1:08:19]  Drive OS, I'm pleased to announce this now the first software defined programmable AI computer that has been certified up to ASLD,
[1:08:19 - 1:08:26]  which is the highest standard of functional safety for automobiles, the only and the highest.
[1:08:27 - 1:08:36]  And so I'm really, really proud of this. ASLD, ISO26262, it is the work of some 15,000 engineering years.
[1:08:37 - 1:08:43]  This is just extraordinary work. And as a result of that, CUDA is now a functional safe computer.
[1:08:44 - 1:08:48]  And so if you're building a robot and video CUDA, yeah.
[1:08:52 - 1:09:03]  Okay, so now I told you I was going to show you what would we use Omniverse and Cosmos to do in the context of self-driving cars.
[1:09:03 - 1:09:11]  And you know, today instead of showing you a whole bunch of videos of cars driving on the road, I'll show you some of that too.
[1:09:12 - 1:09:24]  But I want to show you how we use the car to reconstruct digital twins automatically using AI and use that capability to train future AM models.
[1:09:25 - 1:09:25]  Okay, let's play it.
[1:09:29 - 1:09:49]  The autonomous vehicle revolution is here. Building autonomous vehicles like all robots requires three computers and video DGX to train AI models, Omniverse to test drive and generate synthetic data and drive AGX, a super computer in the car.
[1:09:50 - 1:10:00]  Building safe autonomous vehicles means addressing edge scenarios, but real world data is limited, so synthetic data is essential for training.
[1:10:01 - 1:10:14]  The autonomous vehicle data factory, powered by Envidia Omniverse, AI models and Cosmos, generates synthetic driving scenarios that enhance training data by orders of magnitude.
[1:10:15 - 1:10:23]  First, Omnimap, Fusus, Map and Geospatial Data to construct drivable 3D environments.
[1:10:26 - 1:10:33]  Driving scenario variations can be generated from replay drive logs or AI traffic generators.
[1:10:34 - 1:10:44]  Next, a neural reconstruction engine uses autonomous vehicle sensor logs to create high fidelity for D simulation environments.
[1:10:45 - 1:10:52]  It replays previous drives in 3D and generates scenario variations to amplify training data.
[1:10:53 - 1:10:58]  Finally, edify 3DS automatically searches through existing asset libraries.
[1:10:59 - 1:11:04]  Or, generate new assets to create SIM-ready seeds.
[1:11:07 - 1:11:16]  The Omniverse scenarios are used to condition Cosmos to generate massive amounts of photorealistic data, reducing the SIM to real gap.
[1:11:18 - 1:11:24]  And with text prompts, generate near infinite variations of the driving scenario.
[1:11:25 - 1:11:36]  With Cosmos Nemotron video search, the massively scaled synthetic data set, combined with recorded drives, can be curated to train models.
[1:11:39 - 1:11:49]  Envidia's AI data factory scales hundreds of drives into billions of effective miles, setting the standard for safe and advanced autonomous driving.
[1:11:54 - 1:11:55]  It's not incredible.
[1:11:55 - 1:12:05]  We take thousands of drives and turn them into billions of miles.
[1:12:05 - 1:12:10]  We are going to have mountains of training data for autonomous vehicles.
[1:12:10 - 1:12:13]  Of course, we still need actual cars on the road.
[1:12:13 - 1:12:17]  Of course, we will continuously collect data for as long as we shall live.
[1:12:17 - 1:12:34]  However, synthetic data generation using this multiverse physically-based, physically-grounded capability, so that we generate data for training AI's that are physically grounded and accurate and plausible, so that we can have enormous amount of data trained with.
[1:12:35 - 1:12:36]  The AV industry is here.
[1:12:37 - 1:12:39]  This is an incredibly exciting time.
[1:12:39 - 1:12:43]  Super, super, super excited about the next several years.
[1:12:43 - 1:12:53]  I think you're going to see, just as computer graphics was revolutionized such incredible pace, you're going to see the pace of AV development increasing tremendously over the next several years.
[1:13:02 - 1:13:11]  I think the next part is robotics.
[1:13:12 - 1:13:14]  So, um...
[1:13:19 - 1:13:22]  Give me more robots.
[1:13:25 - 1:13:27]  My friends.
[1:13:33 - 1:13:38]  The chat GPT moment for general robotics is just around the corner.
[1:13:39 - 1:13:52]  And in fact, all of the enabling technologies that I've been talking about is going to make it possible for us in the next several years to see very rapid breakthroughs, surprising breakthroughs in general robotics.
[1:13:52 - 1:14:03]  Now, the reason why general robotics is so important is whereas robots with tracks and wheels require special environments to accommodate them, there are three robots.
[1:14:04 - 1:14:09]  Three robots in the world that we can make that require no green fields.
[1:14:10 - 1:14:13]  Brown field adaptation is perfect.
[1:14:14 - 1:14:20]  If we could possibly build these amazing robots, we could deploy them in exactly the world that we've built for ourselves.
[1:14:21 - 1:14:33]  These three robots are one, agentic robots and agentic AI because, you know, they're information workers so long as they could accommodate the computers that we have in our offices, it's going to be great.
[1:14:34 - 1:14:40]  Number two, self-driving cars. And the reason for that is we spent 100 plus years building roads and cities.
[1:14:41 - 1:14:43]  And then number three, human robots.
[1:14:43 - 1:14:50]  If we have the technology to solve these three, this will be the largest technology industry the world's ever seen.
[1:14:52 - 1:14:56]  And so we think that robotics era is just around the corner.
[1:14:57 - 1:15:01]  The critical capability is how to train these robots.
[1:15:01 - 1:15:08]  In the case of human robots, the imitation information is rather hard to collect.
[1:15:08 - 1:15:13]  And the reason for that is in the case of cars, we're driving cars all the time.
[1:15:13 - 1:15:20]  In the case of these human robots, the imitation information, the human demonstration is rather laborious to do.
[1:15:20 - 1:15:27]  And so we need to come up with a clever way to take hundreds of demonstrations, thousands of human demonstrations.
[1:15:27 - 1:15:43]  And somehow use artificial intelligence and omniverse to synthetically generate millions of synthetically generated motions.
[1:15:43 - 1:15:48]  And from those motions, the AI can learn how to perform a task.
[1:15:48 - 1:15:49]  Let me show you how that's done.
[1:16:01 - 1:16:07]  Developers around the world are building the next wave of physical AI embodied robots, humanoid.
[1:16:08 - 1:16:16]  Developing general purpose robot models requires massive amounts of real world data, which is costly to capture and curate.
[1:16:18 - 1:16:24]  And Vidya Isaac Groot helps tackle these challenges, providing humanoid robot developers with four things.
[1:16:24 - 1:16:34]  Robot foundation models, data pipelines, simulation frameworks, and a Thor robotics computer.
[1:16:35 - 1:16:50]  The Nvidia Isaac Groot blueprint for synthetic motion generation is a simulation workflow for imitation learning, enabling developers to generate exponentially large data sets from a small number of human demonstrations.
[1:16:51 - 1:17:00]  First, Groot Teleop enables skilled human workers to portal into a digital twin of their robot using the Apple Vision Pro.
[1:17:01 - 1:17:11]  This means operators can capture data even without a physical robot, and they can operate the robot in a risk-free environment, eliminating the chance of physical damage or wear and tear.
[1:17:13 - 1:17:21]  To teach a robot a single task, operators capture motion trajectories through a handful of teleoperated demonstrations.
[1:17:21 - 1:17:27]  Then use Groot Mimic to multiply these trajectories into a much larger data set.
[1:17:28 - 1:17:37]  Next, they use Groot Gen built on omniverse and cosmos for domain randomization and 3D to real upscaling.
[1:17:38 - 1:17:51]  Generating an exponentially larger data set, the omniverse and cosmos multiverse simulation engine provides a massively scaled data set to train the robot policy.
[1:17:52 - 1:18:02]  Once the policy is trained, developers can perform software in the loop testing and validation in Isaac Sim before deploying to the real robot.
[1:18:04 - 1:18:09]  The age of general robotics is arriving, powered by NVIDIA Isaac Groot.
[1:18:13 - 1:18:23]  We're going to have mountains of data to train robots with. NVIDIA Isaac Groot, NVIDIA Isaac Groot.
[1:18:23 - 1:18:31]  This is our platform to provide technology elements to the robotics industry to accelerate the development of general robotics.
[1:18:32 - 1:18:44]  I have one more thing that I want to show you. None of this would be possible if not for this incredible project that we started about a decade ago.
[1:18:45 - 1:18:55]  Inside the company was called Project Digits. Deep Learning, GPU, Intelligence, Training System, Digits.
[1:18:56 - 1:19:09]  Before we launched it, I shrunk at the DGX and harmonized it with RTX, AGX, OVX and all of the other X's that we have in the company.
[1:19:12 - 1:19:19]  It really revolutionized, DGX1 really revolutionized, where is DGX1?
[1:19:20 - 1:19:23]  DGX1 revolutionized artificial intelligence.
[1:19:24 - 1:19:32]  The reason why we built it was because we wanted to make it possible for researchers and startups to have an out of the box AI supercomputer.
[1:19:32 - 1:19:42]  Imagine the way super computers were built in the past. You really have to build your own facility and you have to build your own infrastructure and really engineer it into existence.
[1:19:43 - 1:19:50]  We created a super computer for AI, for AI development for researchers and startups that comes literally one out of the box.
[1:19:50 - 1:20:00]  I delivered a first one to a startup company in 2016 called Open AI and Elon was there and Ilya Suskovur was there and many of NVIDIA engineers were there.
[1:20:00 - 1:20:09]  We celebrated the arrival of DGX1 and it revolutionized artificial intelligence computing.
[1:20:10 - 1:20:16]  Now artificial intelligence is everywhere. It's not just in researchers and startup labs.
[1:20:16 - 1:20:19]  We want artificial intelligence as I mentioned in the beginning of our talk.
[1:20:20 - 1:20:23]  This is now the new way of doing computing. This is the new way of doing software.
[1:20:23 - 1:20:34]  Every software engineer, every engineer, every creative artist, everybody who uses computers today as a tool will need AI super computer.
[1:20:35 - 1:20:40]  I just wished that DGX1 was smaller.
[1:20:48 - 1:20:52]  Imagine ladies and gentlemen are...
[1:20:59 - 1:21:04]  This is NVIDIA's latest AI supercomputer.
[1:21:08 - 1:21:13]  And it's finally called Project Digits right now.
[1:21:14 - 1:21:16]  And if you have a good name for it, reach out to us.
[1:21:19 - 1:21:21]  Here's the amazing thing. This is an AI super computer.
[1:21:22 - 1:21:28]  It runs the entire NVIDIA AI stack. All of NVIDIA software runs on this.
[1:21:28 - 1:21:37]  DGX Cloud runs on this. This sits somewhere and it's wireless or connected to your computer.
[1:21:38 - 1:21:42]  It's even a workstation if you like it to be. And you could access it.
[1:21:42 - 1:21:48]  You could reach it like a cloud supercomputer and NVIDIA's AI works on it.
[1:21:49 - 1:21:55]  And it's based on a super secret chip that we've been working on called GB110,
[1:21:55 - 1:21:58]  and the smallest grace blackwell that we make.
[1:21:58 - 1:22:00]  And I have...well...
[1:22:00 - 1:22:02]  Let's show everybody inside.
[1:22:29 - 1:22:32]  Isn't it just...it's just so cute.
[1:22:32 - 1:22:34]  And this is the chip that's inside.
[1:22:35 - 1:22:37]  It is in production.
[1:22:39 - 1:22:43]  This top secret chip we did in collaboration with the CPU, the gray CPU,
[1:22:44 - 1:22:49]  was a...is Bill Fernvidia in collaboration with MediaTek.
[1:22:50 - 1:22:52]  They're the world's leading SOC company.
[1:22:52 - 1:22:55]  And they worked with us to build this CPU, the CPU SOC,
[1:22:55 - 1:23:00]  and connect it with chip-to-chip NVIDLink to the blackwell GPU.
[1:23:01 - 1:23:05]  And this little thing here is in full production.
[1:23:06 - 1:23:11]  We're expecting this computer to be available around May timeframe.
[1:23:12 - 1:23:13]  And so it's coming at you.
[1:23:14 - 1:23:16]  It's just incredible what we could do.
[1:23:16 - 1:23:20]  And it's just...I think it's...you really...
[1:23:22 - 1:23:24]  I was trying to figure out do I need more hands or more pockets?
[1:23:26 - 1:23:30]  All right, so imagine this is what it looks like.
[1:23:31 - 1:23:32]  You know, who doesn't want one of those?
[1:23:34 - 1:23:39]  And if you use PC, Mac, you know, anything.
[1:23:41 - 1:23:43]  Because it's a cloud platform.
[1:23:43 - 1:23:45]  It's a cloud computing platform that sits on your desk.
[1:23:45 - 1:23:48]  You could also use it as a Linux workstation if you like.
[1:23:48 - 1:23:53]  If you would like to have double digits, this is what it looks like.
[1:23:54 - 1:24:01]  And you connect it together with ConnectX and it has Nickel,
[1:24:02 - 1:24:05]  GPU direct, all of that out of the box.
[1:24:06 - 1:24:07]  It's like a supercomputer.
[1:24:07 - 1:24:09]  Our entire supercomputing stack is available.
[1:24:10 - 1:24:13]  And so NVIDIA Project Digits.
[1:24:22 - 1:24:22]  Okay.
[1:24:23 - 1:24:26]  Well, let me tell you what I told you.
[1:24:27 - 1:24:33]  I told you that we are in production with three new black wells.
[1:24:33 - 1:24:39]  Not only is the Grace Blackwell supercomputers in V-Link 72's in production all over the world,
[1:24:39 - 1:24:43]  we now have three new black well systems in production.
[1:24:44 - 1:24:49]  One amazing AI foundational world foundation model.
[1:24:49 - 1:24:51]  The world's first physical AI foundation model.
[1:24:52 - 1:24:57]  It's open, available to activate the world's industries of robotics and such.
[1:24:57 - 1:25:07]  And three and three robotics, three robots working on a genetic AI, human or robots and self-driving cars.
[1:25:09 - 1:25:10]  It's been an incredible year.
[1:25:10 - 1:25:12]  I want to thank all of you for your partnership.
[1:25:13 - 1:25:14]  Thank all of you for coming.
[1:25:14 - 1:25:17]  I made you a short video to reflect on last year and look forward to the next year.
[1:25:18 - 1:25:18]  Play, please.
[1:26:35 - 1:26:37]  I'm going to start with the first one.
